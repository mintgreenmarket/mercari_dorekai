"""
ãƒ©ã‚¯ãƒã®å•†å“ã‚’å‰Šé™¤ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
products_rakuma.csv ã‹ã‚‰å‰Šé™¤å¯¾è±¡ãƒ»é‡è¤‡å¯¾è±¡ã®å•†å“ã‚’èª­ã¿è¾¼ã¿ã€å‰Šé™¤ã™ã‚‹
"""

import os
import re
import shutil
import pandas as pd
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import time

# --- è¨­å®š ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
RAKUMA_CSV = os.path.join(SCRIPT_DIR, 'products_rakuma.csv')
USER_DATA_DIR = os.path.join(SCRIPT_DIR, 'rakuma_user_data_firefox')
PROCESSED_URLS_FILE = os.path.join(SCRIPT_DIR, 'processed_rakuma_urls.txt')

def load_processed_rakuma_urls():
    """å‡¦ç†æ¸ˆã¿ãƒ©ã‚¯ãƒURLã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(PROCESSED_URLS_FILE):
        try:
            with open(PROCESSED_URLS_FILE, 'r', encoding='utf-8') as f:
                processed = set(line.strip() for line in f if line.strip())
            print(f"ğŸ“‹ å‡¦ç†æ¸ˆã¿URL {len(processed)} ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            return processed
        except Exception as e:
            print(f"âš ï¸ å‡¦ç†æ¸ˆã¿URLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return set()
    return set()

def save_processed_rakuma_url(url):
    """å‡¦ç†æ¸ˆã¿URLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜"""
    try:
        with open(PROCESSED_URLS_FILE, 'a', encoding='utf-8') as f:
            f.write(url + '\n')
    except Exception as e:
        print(f"âš ï¸ å‡¦ç†æ¸ˆã¿URLã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def load_target_urls_from_csv():
    """products_rakuma.csv ã‹ã‚‰å‰Šé™¤å¯¾è±¡ãƒ»é‡è¤‡å¯¾è±¡ã®URLã‚’æŠ½å‡ºï¼ˆé‡è¤‡ã¯å¤ã„æ–¹ã‚’å‰Šé™¤ï¼‰"""
    if not os.path.exists(RAKUMA_CSV):
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {RAKUMA_CSV}")
        return []
    
    try:
        df = pd.read_csv(RAKUMA_CSV, encoding='utf-8-sig')
        
        # å‰Šé™¤å¯¾è±¡
        delete_targets = pd.DataFrame()
        if 'å‰Šé™¤' in df.columns:
            delete_targets = df[df['å‰Šé™¤'] == 'å‰Šé™¤'].copy()
        
        # é‡è¤‡å¯¾è±¡ï¼ˆå“ç•ªã”ã¨ã«æ–°ã—ã„1ä»¶ã‚’æ®‹ã—ã¦å¤ã„æ–¹ã‚’å‰Šé™¤ï¼‰
        duplicate_targets = pd.DataFrame()
        if 'é‡è¤‡' in df.columns and 'å“ç•ª' in df.columns:
            dup_df = df[df['é‡è¤‡'] == 'é‡è¤‡'].copy()
            if not dup_df.empty and 'URL' in dup_df.columns:
                date_col = None
                for col in ['æœ€çµ‚æ›´æ–°æ—¥æ™‚', 'å•†å“ç™»éŒ²æ—¥æ™‚']:
                    if col in dup_df.columns:
                        date_col = col
                        break

                dup_df = dup_df.dropna(subset=['URL']).copy()

                if date_col:
                    dup_df['_sort_dt'] = pd.to_datetime(dup_df[date_col], errors='coerce')
                    dup_df['_sort_dt'] = dup_df['_sort_dt'].fillna(pd.Timestamp.min)
                    dup_df = dup_df.sort_values(['å“ç•ª', '_sort_dt', 'URL'], ascending=[True, False, True])
                    dup_df['_dup_rank'] = dup_df.groupby('å“ç•ª').cumcount()
                else:
                    dup_df['_dup_rank'] = dup_df.groupby('å“ç•ª').cumcount()

                duplicate_targets = dup_df[dup_df['_dup_rank'] > 0].copy()
        
        # çµ±åˆ
        if 'URL' in df.columns:
            combined = pd.concat([delete_targets, duplicate_targets], ignore_index=True)
            combined = combined.dropna(subset=['URL']).drop_duplicates(subset=['URL'])
            urls = combined['URL'].tolist()
            
            print(f"ğŸ“¦ å‰Šé™¤å¯¾è±¡: {len(delete_targets)} ä»¶")
            print(f"ğŸ” é‡è¤‡å¯¾è±¡ï¼ˆå¤ã„æ–¹ï¼‰: {len(duplicate_targets)} ä»¶")
            print(f"âœ… åˆè¨ˆ: {len(urls)} ä»¶ã®URLã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
            
            return urls
        else:
            print("âŒ CSVã«'URL'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
            return []
            
    except Exception as e:
        print(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def convert_to_edit_url(url):
    """URLã‚’ç·¨é›†ãƒšãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
    if '/edit' in url:
        return url
    
    if 'item.fril.jp/' in url:
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        base_url = url.split('?')[0]
        # item.fril.jp/{id} ã‹ã‚‰ {id} ã‚’æŠ½å‡º
        parts = base_url.split('item.fril.jp/')
        if len(parts) > 1:
            item_id = parts[1].rstrip('/')
            # fril.jp/item/{id}/edit å½¢å¼ã«å¤‰æ›
            return f"https://fril.jp/item/{item_id}/edit"
    
    return url

def build_url_title_map():
    """CSVã‹ã‚‰URLâ†’å•†å“åã®ãƒãƒƒãƒ—ã‚’ä½œæˆ"""
    url_to_title = {}
    if not os.path.exists(RAKUMA_CSV):
        return url_to_title
    try:
        df = pd.read_csv(RAKUMA_CSV, encoding='utf-8-sig')
        if 'URL' not in df.columns or 'å•†å“å' not in df.columns:
            return url_to_title
        for _, row in df[['URL', 'å•†å“å']].dropna().iterrows():
            raw_url = str(row['URL']).strip()
            title = str(row['å•†å“å']).strip()
            if not raw_url or not title:
                continue
            edit_url = convert_to_edit_url(raw_url)
            url_to_title[raw_url] = title
            url_to_title[edit_url] = title
    except Exception as e:
        print(f"âš ï¸ CSVã‹ã‚‰å•†å“åãƒãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    return url_to_title

def delete_from_drafts(page, title):
    """ä¸‹æ›¸ããƒšãƒ¼ã‚¸ï¼ˆå‡ºå“ã—ã¦ã„ãŸï¼‰ã‹ã‚‰å‰Šé™¤ã‚’è©¦è¡Œ"""
    if not title:
        return False
    if page.is_closed():
        page = page.context.new_page()
    print(f"  â†ª ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã§å‰Šé™¤ã‚’è©¦è¡Œ: {title}")
    try:
        page.goto("https://fril.jp/draft", timeout=60000, wait_until="domcontentloaded")
        page.wait_for_timeout(2000)
    except Exception as e:
        print(f"  âš ï¸ ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã¸ã®ç§»å‹•ã«å¤±æ•—: {e}")
        return False

    tab = page.locator('a[href="#after-selling-tab"]')
    if tab.count() > 0:
        tab.click()
        page.wait_for_timeout(1000)

    def normalize_text(text):
        return "".join(text.split())

    def find_item_by_title(match_title):
        return page.locator(
            "div.deal-item",
            has=page.locator("h4.deal-item__heading", has_text=match_title),
        ).first

    # å“ç•ªãŒã‚ã‚Œã°å“ç•ªå„ªå…ˆã§æ¢ã™
    hinban_match = None
    try:
        hinban_match = re.search(r"(\d{3,5})", title)
    except Exception:
        hinban_match = None
    hinban = hinban_match.group(1) if hinban_match else ""

    search_title = title
    if hinban:
        search_title = hinban

    item = find_item_by_title(search_title)
    if item.count() == 0 and len(title) > 12 and not hinban:
        short_title = title[:12]
        print(f"  âš ï¸ å®Œå…¨ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚çŸ­ç¸®æ¤œç´¢: {short_title}")
        item = find_item_by_title(short_title)

    for _ in range(8):
        if item.count() > 0:
            delete_link = item.locator('a[data-method="delete"]').first
            if delete_link.count() == 0:
                delete_link = item.locator('a:has-text("å‰Šé™¤")').first
            if delete_link.count() == 0:
                print("  âš ï¸ ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã§å‰Šé™¤ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            try:
                delete_link.scroll_into_view_if_needed()
            except Exception:
                pass
            page.once("dialog", lambda d: d.accept())
            delete_link.click(timeout=5000)
            page.wait_for_timeout(2000)
            try:
                item.wait_for(state="detached", timeout=10000)
            except Exception:
                pass
            print("  âœ… ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã§å‰Šé™¤ã—ã¾ã—ãŸ")
            return True

        more_button = page.locator('#after-selling-container_button a')
        if more_button.count() > 0 and more_button.is_visible(timeout=1000):
            more_button.click()
            page.wait_for_timeout(1200)
            item = find_item_by_title(search_title)
            if item.count() == 0 and len(title) > 12 and not hinban:
                item = find_item_by_title(title[:12])
            continue
        break

    print("  âš ï¸ ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã§å¯¾è±¡å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    return False

def delete_products(product_urls):
    """ãƒ©ã‚¯ãƒã®å•†å“ã‚’å‰Šé™¤ã™ã‚‹"""
    if not product_urls:
        print("âœ… å‡¦ç†å¯¾è±¡ã®URLãŒã‚ã‚Šã¾ã›ã‚“")
        return

    url_to_title = build_url_title_map()
    
    # URLã‚’ç·¨é›†ãƒšãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
    # https://item.fril.jp/{id} â†’ https://fril.jp/item/{id}/edit
    edit_urls = []
    for url in product_urls:
        edit_url = convert_to_edit_url(url)
        edit_urls.append(edit_url)
        if edit_url != url:
            print(f"ğŸ“ å¤‰æ›: {url}")
            print(f"    â†’ {edit_url}")
    
    with sync_playwright() as p:
        # Firefoxãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
        print("ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
        browser = p.firefox.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False,
            slow_mo=500
        )
        
        page = browser.pages[0] if browser.pages else browser.new_page()
        
        # æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã®æ©Ÿä¼šã‚’æä¾›
        print("\n" + "=" * 70)
        print("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªã¨æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã®æ™‚é–“")
        print("=" * 70)
        print("ğŸ“Œ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™")
        print("=" * 70)
        
        # ãƒã‚¤ãƒšãƒ¼ã‚¸ã«ç›´æ¥é·ç§»ï¼ˆãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ãªã‚‰ãã®ã¾ã¾è¡¨ç¤ºã€æœªãƒ­ã‚°ã‚¤ãƒ³ãªã‚‰ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ï¼‰
        print("\nğŸŒ ãƒã‚¤ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        try:
            page.goto("https://fril.jp/mypage", timeout=30000)
            page.wait_for_timeout(3000)
            
            # ç¾åœ¨ã®URLã‚’ç¢ºèª
            current_url = page.url
            print(f"ğŸ“ ç¾åœ¨ã®URL: {current_url}")
            
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚ŒãŸå ´åˆ
            if "login" in current_url.lower():
                print("âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚")
                # Persistent Contextã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€é€šå¸¸ã¯è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã•ã‚Œã‚‹ã¯ãš
                # æ•°ç§’å¾…ã£ã¦ã‹ã‚‰å†ç¢ºèª
                page.wait_for_timeout(5000)
                current_url = page.url
                if "login" in current_url.lower():
                    print("âŒ ãƒ­ã‚°ã‚¤ãƒ³ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§ä¸€åº¦æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                    browser.close()
                    return
            
            print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã§ã™")
        except Exception as e:
            print(f"âš ï¸ ãƒšãƒ¼ã‚¸é·ç§»ã‚¨ãƒ©ãƒ¼: {e}")
            print("å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™...")
        
        # ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå®Œäº†ã‚’å¾…ã¤
        print("\nâ³ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã®å®Œäº†ã‚’å¾…ã£ã¦ã„ã¾ã™...")
        page.wait_for_timeout(5000)  # 5ç§’å¾…æ©Ÿã—ã¦ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå®Œäº†ã‚’å¾…ã¤
        
        # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ãŒãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚„ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆä¸­ã§ãªã„ã‹ç¢ºèª
        current_url = page.url
        print(f"ğŸ“ ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®URL: {current_url}")
        
        # ã‚‚ã—ã¾ã ãƒ­ã‚°ã‚¤ãƒ³é–¢é€£ã®URLãªã‚‰ã€ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå®Œäº†ã‚’å¾…ã¤
        if "login" in current_url.lower() or "authorize" in current_url.lower() or "callback" in current_url.lower():
            print("ğŸ”„ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå‡¦ç†ä¸­ã§ã™ã€‚å®Œäº†ã‚’å¾…ã£ã¦ã„ã¾ã™...")
            try:
                # ãƒ­ã‚°ã‚¤ãƒ³é–¢é€£ã®URLã§ãªããªã‚‹ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§30ç§’ï¼‰
                page.wait_for_url(
                    lambda url: "login" not in url.lower() and "authorize" not in url.lower() and "callback" not in url.lower(),
                    timeout=30000
                )
                print("âœ… ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå®Œäº†")
                page.wait_for_timeout(2000)
            except:
                print("âš ï¸ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãã®ã¾ã¾ç¶šè¡Œã—ã¾ã™")
        
        # ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª
        print("\nğŸ” ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
        try:
            page.goto("https://fril.jp/mypage", timeout=60000, wait_until="domcontentloaded")
            page.wait_for_timeout(3000)
        except Exception as e:
            print(f"âš ï¸ ãƒã‚¤ãƒšãƒ¼ã‚¸ã¸ã®é·ç§»ã‚¨ãƒ©ãƒ¼: {e}")
            print("ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã§ç¢ºèªã‚’ç¶šã‘ã¾ã™...")
        
        current_url = page.url
        print(f"ğŸ“ ç¢ºèªURL: {current_url}")
        
        if "login" in current_url.lower():
            print("âŒ ãƒ­ã‚°ã‚¤ãƒ³ãŒç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print("âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            
            # Slacké€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ­ã‚°ã‚¤ãƒ³åˆ‡ã‚Œï¼‰
            try:
                import subprocess
                subprocess.run([
                    r"..\venv\Scripts\python.exe", 
                    "send_slack_notification.py",
                    "âŒ ãƒ©ã‚¯ãƒå‰Šé™¤: ãƒ­ã‚°ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚",
                    "error"
                ], cwd=os.path.dirname(os.path.abspath(__file__)))
            except:
                pass
            
            browser.close()
            return
        
        print("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
        
        # Cookieã‚’ç¢ºèªã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
        cookies = browser.cookies()
        fril_cookies = [c for c in cookies if 'fril.jp' in c.get('domain', '')]
        print(f"ğŸª ãƒ©ã‚¯ãƒã®Cookieæ•°: {len(fril_cookies)}")
        
        if fril_cookies:
            print("âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç¢ºç«‹ã•ã‚Œã¾ã—ãŸ")
            # ä¸»è¦ãªCookieåã‚’è¡¨ç¤º
            cookie_names = [c.get('name', '') for c in fril_cookies]
            print(f"   Cookieå: {', '.join(cookie_names[:5])}")  # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º
        else:
            print("âš ï¸ Cookie ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä¸å®‰å®šãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # å‡¦ç†é–‹å§‹
        print(f"\nğŸ—‘ï¸ {len(edit_urls)} ä»¶ã®å•†å“ã‚’å‰Šé™¤ã—ã¾ã™\n")
        
        success_count = 0
        fail_count = 0
        
        for idx, url in enumerate(edit_urls, 1):
            print(f"[{idx}/{len(edit_urls)}] {url}")
            title = url_to_title.get(url, "")
            
            try:
                # å•†å“ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                try:
                    # ã¾ãšãƒã‚¤ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¢ºèªï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
                    retry_count = 0
                    max_retries = 2
                    
                    while retry_count <= max_retries:
                        try:
                            page.goto("https://fril.jp/mypage", timeout=60000, wait_until="domcontentloaded")
                            page.wait_for_timeout(1000)
                            break
                        except Exception as retry_error:
                            retry_count += 1
                            if retry_count > max_retries:
                                raise retry_error
                            print(f"  ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ {retry_count}/{max_retries}...")
                            page.wait_for_timeout(3000)
                    
                    if "login" in page.url.lower():
                        print("  âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®å•†å“ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    
                    # å•†å“ç·¨é›†ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                    page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    page.wait_for_timeout(3000)
                    
                    # ãƒšãƒ¼ã‚¸é·ç§»å¾Œã®URLã‚’ç¢ºèªã—ã¦ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
                    current_url = page.url
                    print(f"  ğŸ“ ã‚¢ã‚¯ã‚»ã‚¹å…ˆ: {current_url}")
                    
                    # 404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒã‚§ãƒƒã‚¯
                    if page.locator('h1.css-s6ybq1:has-text("ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")').count() > 0:
                        print("  âš ï¸ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå‰Šé™¤æ¸ˆã¿ã¾ãŸã¯ç„¡åŠ¹ãªURLï¼‰")
                        print("  â†’ å‡¦ç†æ¸ˆã¿ã«è¨˜éŒ²ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                        save_processed_rakuma_url(url)
                        success_count += 1
                        continue
                    
                    if "login" in current_url.lower():
                        print("  âŒ ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã¾ã—ãŸ")
                        print("  ğŸ”’ ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„...")
                        # ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’å¾…ã¤ï¼ˆæœ€å¤§60ç§’ï¼‰
                        try:
                            page.wait_for_url(lambda u: "login" not in u.lower() and "edit" in u.lower(), timeout=60000)
                            print("  âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
                            page.wait_for_timeout(2000)
                        except:
                            print("  âŒ ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€ã“ã®å•†å“ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                            fail_count += 1
                            continue
                    
                except Exception as goto_error:
                    print(f"  âŒ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {goto_error}")
                    fail_count += 1
                    continue
                
                # ç·¨é›†ãƒšãƒ¼ã‚¸ã§ã€Œä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹ã€â†’ã€Œç¢ºèªã™ã‚‹ã€â†’ã€Œä¸‹æ›¸ãã«æˆ»ã™ã€
                moved_to_draft = False
                try:
                    draft_button = page.locator('button:has-text("ä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹")').first
                    if draft_button.count() > 0:
                        draft_button.click(timeout=5000)
                        print("  ğŸ“ ã€Œä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯")

                        try:
                            confirm_pre_button = page.locator('button:has-text("ç¢ºèªã™ã‚‹")').first
                            if confirm_pre_button.count() > 0:
                                confirm_pre_button.click(timeout=5000)
                                print("  ğŸ“ ã€Œç¢ºèªã™ã‚‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯")
                                page.wait_for_timeout(1000)
                        except Exception:
                            pass

                        try:
                            confirm_button = page.locator('button:has-text("ä¸‹æ›¸ãã«æˆ»ã™")').first
                            if confirm_button.count() > 0:
                                confirm_button.click(timeout=5000)
                                print("  âœ… ä¸‹æ›¸ãã«ç§»å‹•ã—ã¾ã—ãŸ")
                                moved_to_draft = True
                                page.wait_for_timeout(2000)
                        except Exception:
                            pass
                except Exception:
                    pass

                if delete_from_drafts(page, title):
                    save_processed_rakuma_url(url)
                    success_count += 1
                    continue

                print("  âš ï¸ ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã§ã‚‚å‰Šé™¤ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                fail_count += 1
                continue
                    
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                fail_count += 1
            
            # æ¬¡ã®å•†å“ã¾ã§ã®å¾…æ©Ÿ
            if idx < len(edit_urls):
                page.wait_for_timeout(2000)
        
        print(f"\nğŸ“Š å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} ä»¶ / å¤±æ•— {fail_count} ä»¶")
        
        browser.close()

def main():
    print("=" * 60)
    print("ãƒ©ã‚¯ãƒå•†å“ å‰Šé™¤ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã¯ä¿æŒï¼‰
    cache_dirs = ['cache2', 'shader-cache', 'ShaderCache', 'startupCache', 
                 'GrShaderCache', 'GraphiteDawnCache']
    for cache_dir_name in cache_dirs:
        cache_path = os.path.join(USER_DATA_DIR, cache_dir_name)
        if os.path.exists(cache_path):
            try:
                shutil.rmtree(cache_path)
                print(f"ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {cache_dir_name}")
            except Exception:
                pass
    
    # å‡¦ç†æ¸ˆã¿URLã‚’èª­ã¿è¾¼ã¿
    processed_urls = load_processed_rakuma_urls()
    
    # CSVã‹ã‚‰å¯¾è±¡URLã‚’èª­ã¿è¾¼ã¿
    target_urls = load_target_urls_from_csv()
    
    if not target_urls:
        print("âœ… å‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # URLã‚’ç·¨é›†ãƒšãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
    target_edit_urls = [convert_to_edit_url(url) for url in target_urls]
    
    # æœªå‡¦ç†ã®URLã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    unprocessed_urls = [url for url in target_edit_urls if url not in processed_urls]
    
    if not unprocessed_urls:
        print(f"âœ… ã™ã¹ã¦å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆæ—¢å‡¦ç†: {len(target_edit_urls)} ä»¶ï¼‰")
        return
    
    print(f"\nğŸ“‹ æœªå‡¦ç†: {len(unprocessed_urls)} ä»¶")
    print(f"ğŸ“‹ æ—¢å‡¦ç†: {len(target_edit_urls) - len(unprocessed_urls)} ä»¶")
    
    # å‰Šé™¤
    delete_products(unprocessed_urls)

if __name__ == '__main__':
    main()
